1.书生.浦语2.0 (InternLM2) 的体系
-两个规格
7B:为轻量级的研究和应用提供了一个轻便但性能不俗的模型
20B:模型的综合性能更为强劲，可有效支持更加复杂的实用场景
-每个规格包含三个模型版本(面向不同的使用需求)
InternLM2-Base:模型基座
InternLM2:推荐的在大部分应用中考虑选用的优秀基座
InternLM2-Chat:在Base基础上，经过SFT和RLHF,面向对话交互进行了优化，具有很好的指令遵循、共情聊天和调用工具等能力

2.书生浦语2.0 (InternLM2) 的主要亮点
超长上下文:模型在20万token上下文中，几乎完美实现"大海捞针"
综合性能全面提升:推理、数学、代码提升显著，InternLM2-Chat-20B在重点评测上比肩ChatGPT3.5
优秀的对话和创作体验:精准指令跟随，丰富的结构化创作，在AlpacaEval2上超越GPT-3.5和Gemini Pro
突出的数理能力和实用的数据分析功能:强大的内生计算能力，加入代码解释后，在GSM8K和MATH达到和GPT-4相仿水平(20B)

3.书生浦语开源开放体系
（1）微调：大语言模型的下游应用中，增量续训和有监督微调是经常会用到两种方式。
-增量续训
使用场景:让基座模型学习到一些新知识，如某个垂类领域知识
训练数据:文章、书籍、代码等

-有监督微调
使用场景:让模型学会理解各种指令进行对话，或者注入少量领域知识
训练数据:高质量的对话、问答数据
（2）评测：使用OpenCompass 司南大模型评测体系，CompassKit大模型评测全栈工具链


